import { NextResponse } from "next/server";
import { GoogleGenerativeAI } from "@google/generative-ai";
import { computeStatFeatures, assessStats } from "@/lib/detector";

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY!);
const MODEL = process.env.GEMINI_MODEL || "gemini-1.5-flash-latest";

export async function POST(req: Request) {
  let text = "";
  let features: any;
  let stats: any;
  let isLarge = false;

  try {
    const body = await req.json();
    text = body.text;

    if (!text || text.trim().length < 20) {
      return NextResponse.json(
        { error: "Text too short for analysis" },
        { status: 400 }
      );
    }

    /* -------------------------
       1. Length-based bias for large texts
    -------------------------- */
    const textLength = text.trim().length;
    isLarge = textLength > 700;

    /* -------------------------
       2. Statistical analysis
    -------------------------- */
    features = await computeStatFeatures(text);
    stats = assessStats(features);

    /* -------------------------
       3. Stats-only fallback
    -------------------------- */
    if (!process.env.GEMINI_API_KEY) {
      const baseConf = stats.score * 0.8 + 0.1;
      const boostedConf = isLarge ? Math.min(1, baseConf + 0.2) : baseConf;
      const finalConf = Math.max(0, Math.min(1, boostedConf));
      const finalLabel: "Human" | "AI" = isLarge ? "AI" : stats.label;

      return NextResponse.json({
        label: finalLabel,
        confidence: Number(finalConf.toFixed(3)),
        reasons: isLarge
          ? [
              "Large text volume is typically generated by AI systems",
              ...stats.reasons,
            ]
          : stats.reasons,
        sources: { stats: { score: stats.score }, llm: null },
        features,
      });
    }

    /* -------------------------
       4. Gemini reasoning
    -------------------------- */
    const model = genAI.getGenerativeModel({ model: MODEL });

    const PROMPT = `
You are a forensic linguist specializing in AI vs human authorship detection.

Your task is to decide whether the following text is more likely written by:
- a human
- or a modern generative AI system

This is a probabilistic judgment.

STRICT RULES:
- Do NOT default to Human.
- If human traits are absent and text is generic, neutral, or polished → AI.
- Polished encyclopedic or marketing tone is an AI signal.
- Human writing shows inconsistency, subjectivity, specificity, or lived experience.

STAT FEATURE INTERPRETATION:
- Low sentence-length variance → AI
- High compressibility (gzipRatio) → AI
- Low entropy variation → AI
- Repetitive n-grams → AI
- Personal context + irregular rhythm → Human

DECISION:
- Compare AI vs Human signals
- Choose the dominant class
- Set confidence proportional to dominance

RETURN STRICT JSON ONLY:
{
  "label": "Human" | "AI",
  "confidence": number,
  "reasons": string[]
}

Text:
"""${text}"""

Statistical Features:
${JSON.stringify(features, null, 2)}
`;

    const response = await model.generateContent(PROMPT);
    const output = response.response.text();

    const start = output.indexOf("{");
    const end = output.lastIndexOf("}");
    if (start === -1 || end === -1) {
      throw new Error("Invalid JSON from Gemini");
    }

    const llm = JSON.parse(output.slice(start, end + 1));

    const llmLabel: "Human" | "AI" = llm.label === "AI" ? "AI" : "Human";
    const llmConf = Math.max(0, Math.min(1, Number(llm.confidence ?? 0)));

    /* -------------------------
       5. Symmetric ensemble
    -------------------------- */
    const statScore = Math.max(0, Math.min(1, stats.score));

    // Signed signals
    const llmSignal = llmLabel === "AI" ? llmConf : -llmConf;
    const statSignal = statScore - 0.5;

    const blended = 0.55 * llmSignal + 0.45 * statSignal;
    let finalAI = Math.max(0, Math.min(1, 0.5 + blended));

    // Boost AI confidence for large texts
    if (isLarge) {
      finalAI = Math.min(1, finalAI + 0.15);
    }

    /* -------------------------
       6. Final decision
    -------------------------- */
    let finalLabel: "Human" | "AI";

    if (isLarge) {
      finalLabel = "AI";
    } else if (finalAI >= 0.65) {
      finalLabel = "AI";
    } else if (finalAI <= 0.35) {
      finalLabel = "Human";
    } else {
      finalLabel = statScore > 0.6 ? "AI" : "Human";
    }

    const reasons = [
      ...(isLarge
        ? ["Large text volume typically indicates AI generation"]
        : []),
      ...(Array.isArray(llm.reasons) ? llm.reasons : []),
      ...stats.reasons.slice(0, 3),
    ];

    return NextResponse.json({
      label: finalLabel,
      confidence: Number(
        (finalLabel === "AI" ? finalAI : 1 - finalAI).toFixed(3)
      ),
      reasons,
      sources: {
        stats: { score: statScore },
        llm: { label: llmLabel, confidence: llmConf },
      },
      features,
    });
  } catch (err) {
    console.error(err);

    if (stats) {
      const baseConf = stats.score * 0.8 + 0.1;
      const boostedConf = isLarge ? Math.min(1, baseConf + 0.2) : baseConf;
      const finalConf = Math.max(0, Math.min(1, boostedConf));
      const finalLabel: "Human" | "AI" = isLarge ? "AI" : stats.label;

      return NextResponse.json({
        label: finalLabel,
        confidence: Number(finalConf.toFixed(3)),
        reasons: isLarge
          ? [
              "Large text volume is typically generated by AI systems",
              ...stats.reasons,
            ]
          : stats.reasons,
        sources: { stats: { score: stats.score }, llm: null },
        features,
        warning: "LLM unavailable — stats-only result",
      });
    }

    return NextResponse.json(
      { error: "Failed to analyze content" },
      { status: 500 }
    );
  }
}
